{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f621ba0b",
   "metadata": {
    "papermill": {
     "duration": 0.006257,
     "end_time": "2024-05-06T01:09:32.695355",
     "exception": false,
     "start_time": "2024-05-06T01:09:32.689098",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "code step by step:\n",
    "\n",
    "1. **Import Libraries**: In the first part, we import all the necessary libraries and modules that we'll be using throughout the code. These include libraries for data manipulation (`numpy`, `pandas`, `polars`), machine learning models (`lightgbm`, `catboost`), file handling (`joblib`, `Path`), and other utilities (`gc`, `glob`).\n",
    "\n",
    "2. **Define Classes for Data Processing**: The code defines two classes: `Pipeline` and `Aggregator`. These classes encapsulate methods for data preprocessing and feature engineering, respectively.\n",
    "\n",
    "3. **Define Functions for Data Reading and Feature Engineering**: Several functions are defined to read data files, perform feature engineering, and convert data to a more memory-efficient format. These functions include `read_file`, `read_files`, `feature_eng`, `to_pandas`, and `reduce_mem_usage`.\n",
    "\n",
    "4. **Load Trained Models and Model Metadata**: The code loads trained models (`lgb_models`, `cat_models`) and their associated metadata (`lgb_notebook_info`, `cat_notebook_info`) from disk. These models are later used for making predictions on the test data.\n",
    "\n",
    "5. **Define Test Data Paths and Load Test Data**: Test data paths are defined, and the test data is loaded using the previously defined functions for reading data files. The loaded test data is stored in a dictionary called `data_store`.\n",
    "\n",
    "6. **Perform Feature Engineering on Test Data**: The loaded test data is passed through the feature engineering pipeline (`feature_eng`) to generate features required for making predictions.\n",
    "\n",
    "7. **Generate Predictions**: The `VotingModel` class is used to generate predictions on the test data. This class averages the predictions from multiple individual models to obtain the final prediction probabilities.\n",
    "\n",
    "8. **Save Predictions to Submission File**: The predicted probabilities are saved to a CSV file (`submission.csv`) in the format required for submission. The submission file is based on a sample submission file provided earlier (`sample_submission.csv`).\n",
    "\n",
    "9. **Display Submission DataFrame**: Finally, the submission DataFrame (`df_subm`) is displayed, showing the case IDs and corresponding predicted scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92230791",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-06T01:09:32.708285Z",
     "iopub.status.busy": "2024-05-06T01:09:32.707946Z",
     "iopub.status.idle": "2024-05-06T01:09:37.781359Z",
     "shell.execute_reply": "2024-05-06T01:09:37.780385Z"
    },
    "papermill": {
     "duration": 5.082464,
     "end_time": "2024-05-06T01:09:37.783628",
     "exception": false,
     "start_time": "2024-05-06T01:09:32.701164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib  # Import joblib for saving and loading models\n",
    "from pathlib import Path  # Import Path for working with file paths\n",
    "import gc  # Import gc for garbage collection\n",
    "from glob import glob  # Import glob for file matching\n",
    "import numpy as np  # Import numpy for numerical computing\n",
    "import pandas as pd  # Import pandas for data manipulation\n",
    "import polars as pl  # Import polars for fast data manipulation\n",
    "from sklearn.base import BaseEstimator, RegressorMixin  # Import BaseEstimator and RegressorMixin from sklearn.base\n",
    "from sklearn.metrics import roc_auc_score  # Import roc_auc_score from sklearn.metrics\n",
    "import lightgbm as lgb  # Import lightgbm for gradient boosting\n",
    "\n",
    "import warnings  # Import warnings to ignore warnings\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings\n",
    "\n",
    "ROOT = '/kaggle/input/home-credit-credit-risk-model-stability'  # Define ROOT path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0c599f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T01:09:37.797935Z",
     "iopub.status.busy": "2024-05-06T01:09:37.797374Z",
     "iopub.status.idle": "2024-05-06T01:09:37.809612Z",
     "shell.execute_reply": "2024-05-06T01:09:37.808610Z"
    },
    "papermill": {
     "duration": 0.02146,
     "end_time": "2024-05-06T01:09:37.811555",
     "exception": false,
     "start_time": "2024-05-06T01:09:37.790095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    # Method to set data types for specific columns in a DataFrame\n",
    "    def set_table_dtypes(df):\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "        return df\n",
    "\n",
    "    # Method to handle date columns and calculate time differences\n",
    "    def handle_dates(df):\n",
    "        for col in df.columns:\n",
    "            if col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  # Calculate time differences\n",
    "                df = df.with_columns(pl.col(col).dt.total_days())  # Convert time differences to total days\n",
    "        df = df.drop(\"date_decision\", \"MONTH\")  # Drop unnecessary columns\n",
    "        return df\n",
    "\n",
    "    # Method to filter out columns based on missing values and frequency\n",
    "    def filter_cols(df):\n",
    "        for col in df.columns:\n",
    "            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n",
    "                isnull = df[col].is_null().mean()\n",
    "                if isnull > 0.7:\n",
    "                    df = df.drop(col)  # Drop columns with more than 70% missing values\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n",
    "                freq = df[col].n_unique()\n",
    "                if (freq == 1) | (freq > 200):\n",
    "                    df = df.drop(col)  # Drop columns with only one unique value or more than 200 unique values\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "529f4fd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T01:09:37.823677Z",
     "iopub.status.busy": "2024-05-06T01:09:37.823417Z",
     "iopub.status.idle": "2024-05-06T01:09:37.842387Z",
     "shell.execute_reply": "2024-05-06T01:09:37.841397Z"
    },
    "papermill": {
     "duration": 0.027793,
     "end_time": "2024-05-06T01:09:37.844851",
     "exception": false,
     "start_time": "2024-05-06T01:09:37.817058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Aggregator:\n",
    "    # Method to aggregate numerical features\n",
    "    def num_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]  # Select numerical columns\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Calculate max\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Calculate last\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]  # Calculate mean\n",
    "        expr_median = [pl.median(col).alias(f\"median_{col}\") for col in cols]  # Calculate median\n",
    "        expr_var = [pl.var(col).alias(f\"var_{col}\") for col in cols]  # Calculate variance\n",
    "        return expr_max + expr_last + expr_mean \n",
    "\n",
    "    # Method to aggregate date features\n",
    "    def date_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"D\")]  # Select date columns\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Calculate max\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Calculate last\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]  # Calculate mean\n",
    "        expr_median = [pl.median(col).alias(f\"median_{col}\") for col in cols]  # Calculate median\n",
    "        return expr_max + expr_last + expr_mean \n",
    "\n",
    "    # Method to aggregate string features\n",
    "    def str_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"M\",)]  # Select string columns\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Calculate max\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Calculate last\n",
    "        return expr_max + expr_last\n",
    "\n",
    "    # Method to aggregate other features\n",
    "    def other_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]  # Select other columns\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Calculate max\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Calculate last\n",
    "        return expr_max + expr_last\n",
    "\n",
    "    # Method to aggregate count features\n",
    "    def count_expr(df):\n",
    "        cols = [col for col in df.columns if \"num_group\" in col]  # Select count columns\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Calculate max\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Calculate last\n",
    "        return expr_max + expr_last\n",
    "\n",
    "    # Method to get all aggregation expressions\n",
    "    def get_exprs(df):\n",
    "        exprs = Aggregator.num_expr(df) + \\\n",
    "                Aggregator.date_expr(df) + \\\n",
    "                Aggregator.str_expr(df) + \\\n",
    "                Aggregator.other_expr(df) + \\\n",
    "                Aggregator.count_expr(df)\n",
    "        return exprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c98aa829",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T01:09:37.861345Z",
     "iopub.status.busy": "2024-05-06T01:09:37.860721Z",
     "iopub.status.idle": "2024-05-06T01:09:37.866177Z",
     "shell.execute_reply": "2024-05-06T01:09:37.865359Z"
    },
    "papermill": {
     "duration": 0.015507,
     "end_time": "2024-05-06T01:09:37.867994",
     "exception": false,
     "start_time": "2024-05-06T01:09:37.852487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_file(path, depth=None):\n",
    "    # Read parquet file into a Polars DataFrame\n",
    "    df = pl.read_parquet(path)\n",
    "    # Set table data types using Pipeline method\n",
    "    df = df.pipe(Pipeline.set_table_dtypes)\n",
    "    # Aggregate features if depth is specified\n",
    "    if depth in [1, 2]:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df)) \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70e4fc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T01:09:37.880036Z",
     "iopub.status.busy": "2024-05-06T01:09:37.879768Z",
     "iopub.status.idle": "2024-05-06T01:09:37.885531Z",
     "shell.execute_reply": "2024-05-06T01:09:37.884636Z"
    },
    "papermill": {
     "duration": 0.01382,
     "end_time": "2024-05-06T01:09:37.887352",
     "exception": false,
     "start_time": "2024-05-06T01:09:37.873532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_files(regex_path, depth=None):\n",
    "    chunks = []\n",
    "    # Iterate over files matching the regex pattern\n",
    "    for path in glob(str(regex_path)):\n",
    "        # Read parquet file into a Polars DataFrame\n",
    "        df = pl.read_parquet(path)\n",
    "        # Set table data types using Pipeline method\n",
    "        df = df.pipe(Pipeline.set_table_dtypes)\n",
    "        # Aggregate features if depth is specified\n",
    "        if depth in [1, 2]:\n",
    "            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "        chunks.append(df)\n",
    "    # Concatenate DataFrames and drop duplicate rows based on \"case_id\"\n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\").unique(subset=[\"case_id\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e45c22a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T01:09:37.899488Z",
     "iopub.status.busy": "2024-05-06T01:09:37.899229Z",
     "iopub.status.idle": "2024-05-06T01:09:37.904713Z",
     "shell.execute_reply": "2024-05-06T01:09:37.903926Z"
    },
    "papermill": {
     "duration": 0.013717,
     "end_time": "2024-05-06T01:09:37.906561",
     "exception": false,
     "start_time": "2024-05-06T01:09:37.892844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "    # Add month and weekday features based on \"date_decision\"\n",
    "    df_base = df_base.with_columns(\n",
    "        month_decision = pl.col(\"date_decision\").dt.month(),\n",
    "        weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n",
    "    )\n",
    "    # Join additional depth DataFrames\n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "    # Handle dates using Pipeline method\n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)\n",
    "    return df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bca2a18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T01:09:37.918542Z",
     "iopub.status.busy": "2024-05-06T01:09:37.918290Z",
     "iopub.status.idle": "2024-05-06T01:09:37.922921Z",
     "shell.execute_reply": "2024-05-06T01:09:37.922095Z"
    },
    "papermill": {
     "duration": 0.012803,
     "end_time": "2024-05-06T01:09:37.924826",
     "exception": false,
     "start_time": "2024-05-06T01:09:37.912023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_pandas(df_data, cat_cols=None):\n",
    "    # Convert Polars DataFrame to pandas DataFrame\n",
    "    df_data = df_data.to_pandas()\n",
    "    # Convert categorical columns to category data type\n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "    return df_data, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fbc8863",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T01:09:37.936799Z",
     "iopub.status.busy": "2024-05-06T01:09:37.936569Z",
     "iopub.status.idle": "2024-05-06T01:09:37.948823Z",
     "shell.execute_reply": "2024-05-06T01:09:37.948077Z"
    },
    "papermill": {
     "duration": 0.020272,
     "end_time": "2024-05-06T01:09:37.950581",
     "exception": false,
     "start_time": "2024-05-06T01:09:37.930309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" \n",
    "    Iterate through all the columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2  # Memory usage before optimization\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if str(col_type)==\"category\":\n",
    "            continue\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            continue\n",
    "    end_mem = df.memory_usage().sum() / 1024**2  # Memory usage after optimization\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc25cde6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T01:09:37.962353Z",
     "iopub.status.busy": "2024-05-06T01:09:37.962111Z",
     "iopub.status.idle": "2024-05-06T01:09:38.870526Z",
     "shell.execute_reply": "2024-05-06T01:09:38.869653Z"
    },
    "papermill": {
     "duration": 0.916604,
     "end_time": "2024-05-06T01:09:38.872610",
     "exception": false,
     "start_time": "2024-05-06T01:09:37.956006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- [lgb] notebook_start_time: 2024-04-17 17:19:35.710340\n",
      "- [lgb] description: Add notebook info dict to store cols and cat_cols\n",
      "- [lgb] len(cols): 386\n",
      "- [lgb] len(cat_cols): 113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[LGBMClassifier(colsample_bynode=0.8, colsample_bytree=0.8, device='gpu',\n",
       "                extra_trees=True, learning_rate=0.05, max_depth=10, metric='auc',\n",
       "                n_estimators=2000, num_leaves=64, objective='binary',\n",
       "                random_state=42, reg_alpha=0.1, reg_lambda=10,\n",
       "                sample_weight='balanced', verbose=-1),\n",
       " LGBMClassifier(colsample_bynode=0.8, colsample_bytree=0.8, device='gpu',\n",
       "                extra_trees=True, learning_rate=0.05, max_depth=10, metric='auc',\n",
       "                n_estimators=2000, num_leaves=64, objective='binary',\n",
       "                random_state=42, reg_alpha=0.1, reg_lambda=10,\n",
       "                sample_weight='balanced', verbose=-1),\n",
       " LGBMClassifier(colsample_bynode=0.8, colsample_bytree=0.8, device='gpu',\n",
       "                extra_trees=True, learning_rate=0.05, max_depth=10, metric='auc',\n",
       "                n_estimators=2000, num_leaves=64, objective='binary',\n",
       "                random_state=42, reg_alpha=0.1, reg_lambda=10,\n",
       "                sample_weight='balanced', verbose=-1),\n",
       " LGBMClassifier(colsample_bynode=0.8, colsample_bytree=0.8, device='gpu',\n",
       "                extra_trees=True, learning_rate=0.05, max_depth=10, metric='auc',\n",
       "                n_estimators=2000, num_leaves=64, objective='binary',\n",
       "                random_state=42, reg_alpha=0.1, reg_lambda=10,\n",
       "                sample_weight='balanced', verbose=-1),\n",
       " LGBMClassifier(colsample_bynode=0.8, colsample_bytree=0.8, device='gpu',\n",
       "                extra_trees=True, learning_rate=0.05, max_depth=10, metric='auc',\n",
       "                n_estimators=2000, num_leaves=64, objective='binary',\n",
       "                random_state=42, reg_alpha=0.1, reg_lambda=10,\n",
       "                sample_weight='balanced', verbose=-1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_notebook_info = joblib.load('/kaggle/input/homecredit-models-public/other/lgb/1/notebook_info.joblib')\n",
    "\n",
    "# Print notebook information\n",
    "print(f\"- [lgb] notebook_start_time: {lgb_notebook_info['notebook_start_time']}\")\n",
    "print(f\"- [lgb] description: {lgb_notebook_info['description']}\")\n",
    "\n",
    "# Load columns and categorical columns\n",
    "cols = lgb_notebook_info['cols']\n",
    "cat_cols = lgb_notebook_info['cat_cols']\n",
    "print(f\"- [lgb] len(cols): {len(cols)}\")\n",
    "print(f\"- [lgb] len(cat_cols): {len(cat_cols)}\")\n",
    "\n",
    "# Load LightGBM models\n",
    "lgb_models = joblib.load('/kaggle/input/homecredit-models-public/other/lgb/1/lgb_models.joblib')\n",
    "lgb_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92524302",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T01:09:38.885498Z",
     "iopub.status.busy": "2024-05-06T01:09:38.885226Z",
     "iopub.status.idle": "2024-05-06T01:09:43.601060Z",
     "shell.execute_reply": "2024-05-06T01:09:43.600032Z"
    },
    "papermill": {
     "duration": 4.724808,
     "end_time": "2024-05-06T01:09:43.603379",
     "exception": false,
     "start_time": "2024-05-06T01:09:38.878571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- [cat] notebook_start_time: 2024-04-18 00:37:32.864485\n",
      "- [cat] description: first cat models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<catboost.core.CatBoostClassifier at 0x7cda6a3d3bb0>,\n",
       " <catboost.core.CatBoostClassifier at 0x7cda696f17e0>,\n",
       " <catboost.core.CatBoostClassifier at 0x7cda65c9e0e0>,\n",
       " <catboost.core.CatBoostClassifier at 0x7cda6522e9b0>,\n",
       " <catboost.core.CatBoostClassifier at 0x7cda649bb280>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load categorical model notebook information\n",
    "cat_notebook_info = joblib.load('/kaggle/input/homecredit-models-public/other/cat/1/notebook_info.joblib')\n",
    "\n",
    "# Print notebook information\n",
    "print(f\"- [cat] notebook_start_time: {cat_notebook_info['notebook_start_time']}\")\n",
    "print(f\"- [cat] description: {cat_notebook_info['description']}\")\n",
    "\n",
    "# Load categorical models\n",
    "cat_models = joblib.load('/kaggle/input/homecredit-models-public/other/cat/1/cat_models.joblib')\n",
    "cat_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7be1d84f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T01:09:43.618040Z",
     "iopub.status.busy": "2024-05-06T01:09:43.617723Z",
     "iopub.status.idle": "2024-05-06T01:09:44.063665Z",
     "shell.execute_reply": "2024-05-06T01:09:44.062577Z"
    },
    "papermill": {
     "duration": 0.455929,
     "end_time": "2024-05-06T01:09:44.066108",
     "exception": false,
     "start_time": "2024-05-06T01:09:43.610179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the root directory path\n",
    "ROOT = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n",
    "\n",
    "# Define the directory path for the test data\n",
    "TEST_DIR = ROOT / \"parquet_files\" / \"test\"\n",
    "\n",
    "# Create a dictionary to store different dataframes generated from reading parquet files\n",
    "data_store = {\n",
    "    # Read the base test data and store it with the key 'df_base'\n",
    "    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n",
    "    \n",
    "    # Read depth 0 data, which includes static data and additional files matching a pattern\n",
    "    \"depth_0\": [\n",
    "        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n",
    "        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \n",
    "    # Read depth 1 data, including various files related to applicant previous applications, tax registries,\n",
    "    # credit bureau data, and other information\n",
    "    \"depth_1\": [\n",
    "        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \n",
    "    # Read depth 2 data, which includes additional credit bureau data, applicant previous applications,\n",
    "    # and personal information\n",
    "    \"depth_2\": [\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n",
    "        read_file(TEST_DIR / \"test_applprev_2.parquet\", 2),\n",
    "        read_file(TEST_DIR / \"test_person_2.parquet\", 2)\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f50eacda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T01:09:44.081486Z",
     "iopub.status.busy": "2024-05-06T01:09:44.080765Z",
     "iopub.status.idle": "2024-05-06T01:09:44.617770Z",
     "shell.execute_reply": "2024-05-06T01:09:44.616882Z"
    },
    "papermill": {
     "duration": 0.546324,
     "end_time": "2024-05-06T01:09:44.619600",
     "exception": false,
     "start_time": "2024-05-06T01:09:44.073276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data shape:\t (10, 860)\n",
      "Memory usage of dataframe is 0.04 MB\n",
      "Memory usage after optimization is: 0.02 MB\n",
      "Decreased by 40.2%\n",
      "test data shape:\t (10, 386)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform feature engineering on the test data using the provided data store\n",
    "df_test = feature_eng(**data_store)\n",
    "\n",
    "# Print the shape of the test data before further processing\n",
    "print(\"test data shape:\\t\", df_test.shape)\n",
    "\n",
    "# Clean up memory by deleting the data store and running garbage collection\n",
    "del data_store\n",
    "gc.collect()\n",
    "\n",
    "# Select columns of interest from the test data\n",
    "df_test = df_test.select(['case_id'] + cols)\n",
    "\n",
    "# Convert the test data to a pandas DataFrame and optimize memory usage\n",
    "df_test, cat_cols = to_pandas(df_test, cat_cols)\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "\n",
    "# Set the case_id column as the index of the DataFrame\n",
    "df_test = df_test.set_index('case_id')\n",
    "\n",
    "# Print the shape of the test data after processing\n",
    "print(\"test data shape:\\t\", df_test.shape)\n",
    "\n",
    "# Run garbage collection to clean up memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dba9ee8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T01:09:44.633365Z",
     "iopub.status.busy": "2024-05-06T01:09:44.633104Z",
     "iopub.status.idle": "2024-05-06T01:09:44.670604Z",
     "shell.execute_reply": "2024-05-06T01:09:44.669758Z"
    },
    "papermill": {
     "duration": 0.046589,
     "end_time": "2024-05-06T01:09:44.672637",
     "exception": false,
     "start_time": "2024-05-06T01:09:44.626048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_decision</th>\n",
       "      <th>weekday_decision</th>\n",
       "      <th>credamount_770A</th>\n",
       "      <th>applicationcnt_361L</th>\n",
       "      <th>applications30d_658L</th>\n",
       "      <th>applicationscnt_1086L</th>\n",
       "      <th>applicationscnt_464L</th>\n",
       "      <th>applicationscnt_867L</th>\n",
       "      <th>clientscnt_1022L</th>\n",
       "      <th>clientscnt_100L</th>\n",
       "      <th>...</th>\n",
       "      <th>max_cacccardblochreas_147M</th>\n",
       "      <th>last_cacccardblochreas_147M</th>\n",
       "      <th>max_conts_type_509L</th>\n",
       "      <th>last_conts_type_509L</th>\n",
       "      <th>max_conts_role_79M</th>\n",
       "      <th>max_empls_economicalst_849M</th>\n",
       "      <th>max_empls_employer_name_740M</th>\n",
       "      <th>last_conts_role_79M</th>\n",
       "      <th>last_empls_economicalst_849M</th>\n",
       "      <th>last_empls_employer_name_740M</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57543</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>PRIMARY_MOBILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57549</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>75000.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57551</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>27095.201172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>a55475b1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57552</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>a55475b1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57569</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>a55475b1</td>\n",
       "      <td>a55475b1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57630</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>96174.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57631</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>24920.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57632</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>25998.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57633</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57634</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12108.200195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 386 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         month_decision  weekday_decision  credamount_770A  \\\n",
       "case_id                                                      \n",
       "57543                 5                 5     20000.000000   \n",
       "57549                 1                 1     75000.000000   \n",
       "57551                11                 5     27095.201172   \n",
       "57552                11                 5    100000.000000   \n",
       "57569                12                 1     60000.000000   \n",
       "57630                 3                 2     96174.000000   \n",
       "57631                 6                 6     24920.000000   \n",
       "57632                 2                 6     25998.000000   \n",
       "57633                 1                 2    200000.000000   \n",
       "57634                 1                 3     12108.200195   \n",
       "\n",
       "         applicationcnt_361L  applications30d_658L  applicationscnt_1086L  \\\n",
       "case_id                                                                     \n",
       "57543                    0.0                   0.0                    0.0   \n",
       "57549                    0.0                   2.0                    0.0   \n",
       "57551                    0.0                   1.0                    0.0   \n",
       "57552                    0.0                   0.0                    0.0   \n",
       "57569                    0.0                   1.0                    0.0   \n",
       "57630                    0.0                   0.0                    0.0   \n",
       "57631                    0.0                   0.0                    0.0   \n",
       "57632                    0.0                   0.0                    0.0   \n",
       "57633                    0.0                   0.0                    0.0   \n",
       "57634                    0.0                   0.0                    0.0   \n",
       "\n",
       "         applicationscnt_464L  applicationscnt_867L  clientscnt_1022L  \\\n",
       "case_id                                                                 \n",
       "57543                     0.0                   9.0               0.0   \n",
       "57549                     0.0                  10.0               0.0   \n",
       "57551                     0.0                   2.0               0.0   \n",
       "57552                     0.0                   9.0               0.0   \n",
       "57569                     0.0                   6.0               0.0   \n",
       "57630                     0.0                   1.0               0.0   \n",
       "57631                     0.0                   0.0               0.0   \n",
       "57632                     0.0                   1.0               0.0   \n",
       "57633                     0.0                   3.0               0.0   \n",
       "57634                     0.0                   0.0               0.0   \n",
       "\n",
       "         clientscnt_100L  ...  max_cacccardblochreas_147M  \\\n",
       "case_id                   ...                               \n",
       "57543                0.0  ...                    a55475b1   \n",
       "57549                0.0  ...                         NaN   \n",
       "57551                0.0  ...                         NaN   \n",
       "57552                0.0  ...                         NaN   \n",
       "57569                0.0  ...                         NaN   \n",
       "57630                0.0  ...                         NaN   \n",
       "57631                0.0  ...                         NaN   \n",
       "57632                0.0  ...                         NaN   \n",
       "57633                0.0  ...                         NaN   \n",
       "57634                0.0  ...                         NaN   \n",
       "\n",
       "         last_cacccardblochreas_147M  max_conts_type_509L  \\\n",
       "case_id                                                     \n",
       "57543                       a55475b1       PRIMARY_MOBILE   \n",
       "57549                            NaN                  NaN   \n",
       "57551                            NaN                  NaN   \n",
       "57552                            NaN                  NaN   \n",
       "57569                            NaN                  NaN   \n",
       "57630                            NaN                  NaN   \n",
       "57631                            NaN                  NaN   \n",
       "57632                            NaN                  NaN   \n",
       "57633                            NaN                  NaN   \n",
       "57634                            NaN                  NaN   \n",
       "\n",
       "         last_conts_type_509L  max_conts_role_79M  \\\n",
       "case_id                                             \n",
       "57543                     NaN                 NaN   \n",
       "57549                     NaN                 NaN   \n",
       "57551                     NaN            a55475b1   \n",
       "57552                     NaN            a55475b1   \n",
       "57569                     NaN            a55475b1   \n",
       "57630                     NaN                 NaN   \n",
       "57631                     NaN                 NaN   \n",
       "57632                     NaN                 NaN   \n",
       "57633                     NaN                 NaN   \n",
       "57634                     NaN                 NaN   \n",
       "\n",
       "         max_empls_economicalst_849M  max_empls_employer_name_740M  \\\n",
       "case_id                                                              \n",
       "57543                            NaN                           NaN   \n",
       "57549                            NaN                           NaN   \n",
       "57551                       a55475b1                      a55475b1   \n",
       "57552                       a55475b1                      a55475b1   \n",
       "57569                       a55475b1                      a55475b1   \n",
       "57630                            NaN                           NaN   \n",
       "57631                            NaN                           NaN   \n",
       "57632                            NaN                           NaN   \n",
       "57633                            NaN                           NaN   \n",
       "57634                            NaN                           NaN   \n",
       "\n",
       "         last_conts_role_79M  last_empls_economicalst_849M  \\\n",
       "case_id                                                      \n",
       "57543                    NaN                           NaN   \n",
       "57549                    NaN                           NaN   \n",
       "57551               a55475b1                      a55475b1   \n",
       "57552               a55475b1                      a55475b1   \n",
       "57569               a55475b1                      a55475b1   \n",
       "57630                    NaN                           NaN   \n",
       "57631                    NaN                           NaN   \n",
       "57632                    NaN                           NaN   \n",
       "57633                    NaN                           NaN   \n",
       "57634                    NaN                           NaN   \n",
       "\n",
       "         last_empls_employer_name_740M  \n",
       "case_id                                 \n",
       "57543                              NaN  \n",
       "57549                              NaN  \n",
       "57551                         a55475b1  \n",
       "57552                         a55475b1  \n",
       "57569                         a55475b1  \n",
       "57630                              NaN  \n",
       "57631                              NaN  \n",
       "57632                              NaN  \n",
       "57633                              NaN  \n",
       "57634                              NaN  \n",
       "\n",
       "[10 rows x 386 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee1e0024",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T01:09:44.688559Z",
     "iopub.status.busy": "2024-05-06T01:09:44.688293Z",
     "iopub.status.idle": "2024-05-06T01:09:44.696625Z",
     "shell.execute_reply": "2024-05-06T01:09:44.695825Z"
    },
    "papermill": {
     "duration": 0.018029,
     "end_time": "2024-05-06T01:09:44.698476",
     "exception": false,
     "start_time": "2024-05-06T01:09:44.680447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VotingModel(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, estimators):\n",
    "        super().__init__()\n",
    "        self.estimators = estimators\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the VotingModel.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: array-like or sparse matrix of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        - y: array-like of shape (n_samples,), default=None\n",
    "            The target values.\n",
    "            \n",
    "        Returns:\n",
    "        - self: object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict regression target for X.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: array-like or sparse matrix of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "            \n",
    "        Returns:\n",
    "        - y_preds: array-like of shape (n_samples,)\n",
    "            The predicted target values.\n",
    "        \"\"\"\n",
    "        y_preds = [estimator.predict(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)\n",
    "     \n",
    "    def predict_proba(self, X):      \n",
    "        \"\"\"\n",
    "        Predict class probabilities for X.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: array-like or sparse matrix of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "            \n",
    "        Returns:\n",
    "        - proba: array-like of shape (n_samples, n_classes)\n",
    "            Class probabilities of the input samples.\n",
    "        \"\"\"\n",
    "        # lgb\n",
    "        y_preds = [estimator.predict_proba(X) for estimator in self.estimators[:5]]\n",
    "        \n",
    "        # cat        \n",
    "        X[cat_cols] = X[cat_cols].astype(str)\n",
    "        y_preds += [estimator.predict_proba(X) for estimator in self.estimators[-5:]]\n",
    "        \n",
    "        return np.mean(y_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1590acb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T01:09:44.713307Z",
     "iopub.status.busy": "2024-05-06T01:09:44.713051Z",
     "iopub.status.idle": "2024-05-06T01:09:44.718340Z",
     "shell.execute_reply": "2024-05-06T01:09:44.717515Z"
    },
    "papermill": {
     "duration": 0.014839,
     "end_time": "2024-05-06T01:09:44.720150",
     "exception": false,
     "start_time": "2024-05-06T01:09:44.705311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VotingModel(lgb_models + cat_models)\n",
    "len(model.estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e92e9081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T01:09:44.735201Z",
     "iopub.status.busy": "2024-05-06T01:09:44.734933Z",
     "iopub.status.idle": "2024-05-06T01:09:45.271297Z",
     "shell.execute_reply": "2024-05-06T01:09:45.270375Z"
    },
    "papermill": {
     "duration": 0.546401,
     "end_time": "2024-05-06T01:09:45.273503",
     "exception": false,
     "start_time": "2024-05-06T01:09:44.727102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57543</th>\n",
       "      <td>0.011151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57549</th>\n",
       "      <td>0.051655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57551</th>\n",
       "      <td>0.002948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57552</th>\n",
       "      <td>0.018932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57569</th>\n",
       "      <td>0.103931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57630</th>\n",
       "      <td>0.008487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57631</th>\n",
       "      <td>0.030032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57632</th>\n",
       "      <td>0.008360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57633</th>\n",
       "      <td>0.028530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57634</th>\n",
       "      <td>0.029617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            score\n",
       "case_id          \n",
       "57543    0.011151\n",
       "57549    0.051655\n",
       "57551    0.002948\n",
       "57552    0.018932\n",
       "57569    0.103931\n",
       "57630    0.008487\n",
       "57631    0.030032\n",
       "57632    0.008360\n",
       "57633    0.028530\n",
       "57634    0.029617"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict probabilities for the test data\n",
    "y_pred = pd.Series(model.predict_proba(df_test)[:, 1], index=df_test.index)\n",
    "\n",
    "# Read the sample submission file\n",
    "df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n",
    "df_subm = df_subm.set_index(\"case_id\")\n",
    "\n",
    "# Assign predicted probabilities to the submission dataframe\n",
    "df_subm[\"score\"] = y_pred\n",
    "\n",
    "# Save the submission dataframe to a CSV file\n",
    "df_subm.to_csv(\"submission.csv\")\n",
    "\n",
    "# Display the submission dataframe\n",
    "df_subm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bd5795",
   "metadata": {
    "papermill": {
     "duration": 0.007156,
     "end_time": "2024-05-06T01:09:45.288257",
     "exception": false,
     "start_time": "2024-05-06T01:09:45.281101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7921029,
     "sourceId": 50160,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 27710,
     "sourceId": 33095,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 27711,
     "sourceId": 33096,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15.924031,
   "end_time": "2024-05-06T01:09:45.914760",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-06T01:09:29.990729",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
