{"cells":[{"cell_type":"markdown","metadata":{},"source":["code step by step:\n","\n","1. **Import Libraries**: In the first part, we import all the necessary libraries and modules that we'll be using throughout the code. These include libraries for data manipulation (`numpy`, `pandas`, `polars`), machine learning models (`lightgbm`, `catboost`), file handling (`joblib`, `Path`), and other utilities (`gc`, `glob`).\n","\n","2. **Define Classes for Data Processing**: The code defines two classes: `Pipeline` and `Aggregator`. These classes encapsulate methods for data preprocessing and feature engineering, respectively.\n","\n","3. **Define Functions for Data Reading and Feature Engineering**: Several functions are defined to read data files, perform feature engineering, and convert data to a more memory-efficient format. These functions include `read_file`, `read_files`, `feature_eng`, `to_pandas`, and `reduce_mem_usage`.\n","\n","4. **Load Trained Models and Model Metadata**: The code loads trained models (`lgb_models`, `cat_models`) and their associated metadata (`lgb_notebook_info`, `cat_notebook_info`) from disk. These models are later used for making predictions on the train data.\n","\n","5. **Define train Data Paths and Load train Data**: train data paths are defined, and the train data is loaded using the previously defined functions for reading data files. The loaded train data is stored in a dictionary called `data_store`.\n","\n","6. **Perform Feature Engineering on train Data**: The loaded train data is passed through the feature engineering pipeline (`feature_eng`) to generate features required for making predictions.\n","\n","7. **Generate Predictions**: The `VotingModel` class is used to generate predictions on the train data. This class averages the predictions from multiple individual models to obtain the final prediction probabilities.\n","\n","8. **Save Predictions to Submission File**: The predicted probabilities are saved to a CSV file (`submission.csv`) in the format required for submission. The submission file is based on a sample submission file provided earlier (`sample_submission.csv`).\n","\n","9. **Display Submission DataFrame**: Finally, the submission DataFrame (`df_subm`) is displayed, showing the case IDs and corresponding predicted scores.\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-20T16:49:38.512375Z","iopub.status.busy":"2024-04-20T16:49:38.511999Z","iopub.status.idle":"2024-04-20T16:49:43.683354Z","shell.execute_reply":"2024-04-20T16:49:43.682352Z","shell.execute_reply.started":"2024-04-20T16:49:38.512324Z"},"papermill":{"duration":6.044727,"end_time":"2024-04-18T01:05:56.081208","exception":false,"start_time":"2024-04-18T01:05:50.036481","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import joblib  # Import joblib for saving and loading models\n","from pathlib import Path  # Import Path for working with file paths\n","import gc  # Import gc for garbage collection\n","from glob import glob  # Import glob for file matching\n","import numpy as np  # Import numpy for numerical computing\n","import pandas as pd  # Import pandas for data manipulation\n","import polars as pl  # Import polars for fast data manipulation\n","from sklearn.base import BaseEstimator, RegressorMixin  # Import BaseEstimator and RegressorMixin from sklearn.base\n","from sklearn.metrics import roc_auc_score  # Import roc_auc_score from sklearn.metrics\n","import lightgbm as lgb  # Import lightgbm for gradient boosting\n","\n","import warnings  # Import warnings to ignore warnings\n","warnings.filterwarnings('ignore')  # Ignore warnings\n","\n","USER = ''\n","ROOT = Path(f\"/home/{USER}/public/home-credit-credit-risk-model-stability\")  # Define ROOT path"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:49:43.685481Z","iopub.status.busy":"2024-04-20T16:49:43.685157Z","iopub.status.idle":"2024-04-20T16:49:43.697671Z","shell.execute_reply":"2024-04-20T16:49:43.696751Z","shell.execute_reply.started":"2024-04-20T16:49:43.685444Z"},"papermill":{"duration":0.034893,"end_time":"2024-04-18T01:05:56.121806","exception":false,"start_time":"2024-04-18T01:05:56.086913","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class Pipeline:\n","    # Method to set data types for specific columns in a DataFrame\n","    def set_table_dtypes(df):\n","        for col in df.columns:\n","            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.Int64))\n","            elif col in [\"date_decision\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.Date))\n","            elif col[-1] in (\"P\", \"A\"):\n","                df = df.with_columns(pl.col(col).cast(pl.Float64))\n","            elif col[-1] in (\"M\",):\n","                df = df.with_columns(pl.col(col).cast(pl.String))\n","            elif col[-1] in (\"D\",):\n","                df = df.with_columns(pl.col(col).cast(pl.Date))\n","        return df\n","\n","    # Method to handle date columns and calculate time differences\n","    def handle_dates(df):\n","        for col in df.columns:\n","            if col[-1] in (\"D\",):\n","                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  # Calculate time differences\n","                df = df.with_columns(pl.col(col).dt.total_days())  # Convert time differences to total days\n","        df = df.drop(\"date_decision\", \"MONTH\")  # Drop unnecessary columns\n","        return df\n","\n","    # Method to filter out columns based on missing values and frequency\n","    def filter_cols(df):\n","        for col in df.columns:\n","            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n","                isnull = df[col].is_null().mean()\n","                if isnull > 0.7:\n","                    df = df.drop(col)  # Drop columns with more than 70% missing values\n","        \n","        for col in df.columns:\n","            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n","                freq = df[col].n_unique()\n","                if (freq == 1) | (freq > 200):\n","                    df = df.drop(col)  # Drop columns with only one unique value or more than 200 unique values\n","        \n","        return df"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:49:43.69932Z","iopub.status.busy":"2024-04-20T16:49:43.698983Z","iopub.status.idle":"2024-04-20T16:49:43.717216Z","shell.execute_reply":"2024-04-20T16:49:43.716319Z","shell.execute_reply.started":"2024-04-20T16:49:43.699297Z"},"trusted":true},"outputs":[],"source":["class Aggregator:\n","    # Method to aggregate numerical features\n","    def num_expr(df):\n","        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]  # Select numerical columns\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Calculate max\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Calculate last\n","        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]  # Calculate mean\n","        expr_median = [pl.median(col).alias(f\"median_{col}\") for col in cols]  # Calculate median\n","        expr_var = [pl.var(col).alias(f\"var_{col}\") for col in cols]  # Calculate variance\n","        return expr_max + expr_last + expr_mean \n","\n","    # Method to aggregate date features\n","    def date_expr(df):\n","        cols = [col for col in df.columns if col[-1] in (\"D\")]  # Select date columns\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Calculate max\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Calculate last\n","        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]  # Calculate mean\n","        expr_median = [pl.median(col).alias(f\"median_{col}\") for col in cols]  # Calculate median\n","        return expr_max + expr_last + expr_mean \n","\n","    # Method to aggregate string features\n","    def str_expr(df):\n","        cols = [col for col in df.columns if col[-1] in (\"M\",)]  # Select string columns\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Calculate max\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Calculate last\n","        return expr_max + expr_last\n","\n","    # Method to aggregate other features\n","    def other_expr(df):\n","        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]  # Select other columns\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Calculate max\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Calculate last\n","        return expr_max + expr_last\n","\n","    # Method to aggregate count features\n","    def count_expr(df):\n","        cols = [col for col in df.columns if \"num_group\" in col]  # Select count columns\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Calculate max\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Calculate last\n","        return expr_max + expr_last\n","\n","    # Method to get all aggregation expressions\n","    def get_exprs(df):\n","        exprs = Aggregator.num_expr(df) + \\\n","                Aggregator.date_expr(df) + \\\n","                Aggregator.str_expr(df) + \\\n","                Aggregator.other_expr(df) + \\\n","                Aggregator.count_expr(df)\n","        return exprs"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:49:43.719527Z","iopub.status.busy":"2024-04-20T16:49:43.719222Z","iopub.status.idle":"2024-04-20T16:49:43.730492Z","shell.execute_reply":"2024-04-20T16:49:43.729674Z","shell.execute_reply.started":"2024-04-20T16:49:43.719505Z"},"papermill":{"duration":0.032096,"end_time":"2024-04-18T01:05:56.159326","exception":false,"start_time":"2024-04-18T01:05:56.12723","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def read_file(path, depth=None):\n","    # Read parquet file into a Polars DataFrame\n","    df = pl.read_parquet(path)\n","    # Set table data types using Pipeline method\n","    df = df.pipe(Pipeline.set_table_dtypes)\n","    # Aggregate features if depth is specified\n","    if depth in [1, 2]:\n","        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df)) \n","    return df\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:49:43.731815Z","iopub.status.busy":"2024-04-20T16:49:43.731533Z","iopub.status.idle":"2024-04-20T16:49:43.74083Z","shell.execute_reply":"2024-04-20T16:49:43.740124Z","shell.execute_reply.started":"2024-04-20T16:49:43.731792Z"},"trusted":true},"outputs":[],"source":["def read_files(regex_path, depth=None):\n","    chunks = []\n","    # Iterate over files matching the regex pattern\n","    for path in glob(str(regex_path)):\n","        # Read parquet file into a Polars DataFrame\n","        df = pl.read_parquet(path)\n","        # Set table data types using Pipeline method\n","        df = df.pipe(Pipeline.set_table_dtypes)\n","        # Aggregate features if depth is specified\n","        if depth in [1, 2]:\n","            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n","        chunks.append(df)\n","    # Concatenate DataFrames and drop duplicate rows based on \"case_id\"\n","    df = pl.concat(chunks, how=\"vertical_relaxed\").unique(subset=[\"case_id\"])\n","    return df"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:49:43.742044Z","iopub.status.busy":"2024-04-20T16:49:43.741783Z","iopub.status.idle":"2024-04-20T16:49:43.751152Z","shell.execute_reply":"2024-04-20T16:49:43.750207Z","shell.execute_reply.started":"2024-04-20T16:49:43.742012Z"},"trusted":true},"outputs":[],"source":["def feature_eng(df_base, depth_0, depth_1, depth_2):\n","    # Add month and weekday features based on \"date_decision\"\n","    df_base = df_base.with_columns(\n","        month_decision = pl.col(\"date_decision\").dt.month(),\n","        weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n","    )\n","    # Join additional depth DataFrames\n","    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n","        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n","    # Handle dates using Pipeline method\n","    df_base = df_base.pipe(Pipeline.handle_dates)\n","    return df_base"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:49:43.752612Z","iopub.status.busy":"2024-04-20T16:49:43.752272Z","iopub.status.idle":"2024-04-20T16:49:43.764564Z","shell.execute_reply":"2024-04-20T16:49:43.763682Z","shell.execute_reply.started":"2024-04-20T16:49:43.752588Z"},"trusted":true},"outputs":[],"source":["def to_pandas(df_data, cat_cols=None):\n","    # Convert Polars DataFrame to pandas DataFrame\n","    df_data = df_data.to_pandas()\n","    # Convert categorical columns to category data type\n","    if cat_cols is None:\n","        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n","    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n","    return df_data, cat_cols"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:49:43.766024Z","iopub.status.busy":"2024-04-20T16:49:43.765726Z","iopub.status.idle":"2024-04-20T16:49:43.779552Z","shell.execute_reply":"2024-04-20T16:49:43.778709Z","shell.execute_reply.started":"2024-04-20T16:49:43.766001Z"},"trusted":true},"outputs":[],"source":["def reduce_mem_usage(df):\n","    \"\"\" \n","    Iterate through all the columns of a dataframe and modify the data type\n","    to reduce memory usage.\n","    \"\"\"\n","    start_mem = df.memory_usage().sum() / 1024**2  # Memory usage before optimization\n","    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n","    \n","    for col in df.columns:\n","        col_type = df[col].dtype\n","        if str(col_type)==\"category\":\n","            continue\n","        \n","        if col_type != object:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if str(col_type)[:3] == 'int':\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)  \n","            else:\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float16)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)\n","        else:\n","            continue\n","    end_mem = df.memory_usage().sum() / 1024**2  # Memory usage after optimization\n","    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n","    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n","    \n","    return df"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:49:43.780841Z","iopub.status.busy":"2024-04-20T16:49:43.780587Z","iopub.status.idle":"2024-04-20T16:49:44.792315Z","shell.execute_reply":"2024-04-20T16:49:44.791459Z","shell.execute_reply.started":"2024-04-20T16:49:43.780819Z"},"papermill":{"duration":0.722458,"end_time":"2024-04-18T01:05:56.898184","exception":false,"start_time":"2024-04-18T01:05:56.175726","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# lgb_notebook_info = joblib.load('/kaggle/input/homecredit-models-public/other/lgb/1/notebook_info.joblib')\n","\n","# # Print notebook information\n","# print(f\"- [lgb] notebook_start_time: {lgb_notebook_info['notebook_start_time']}\")\n","# print(f\"- [lgb] description: {lgb_notebook_info['description']}\")\n","\n","# # Load columns and categorical columns\n","# cols = lgb_notebook_info['cols']\n","# cat_cols = lgb_notebook_info['cat_cols']\n","# print(f\"- [lgb] len(cols): {len(cols)}\")\n","# print(f\"- [lgb] len(cat_cols): {len(cat_cols)}\")\n","\n","# # Load LightGBM models\n","# lgb_models = joblib.load('/kaggle/input/homecredit-models-public/other/lgb/1/lgb_models.joblib')\n","# lgb_models"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:49:44.796015Z","iopub.status.busy":"2024-04-20T16:49:44.795724Z","iopub.status.idle":"2024-04-20T16:49:49.486001Z","shell.execute_reply":"2024-04-20T16:49:49.485113Z","shell.execute_reply.started":"2024-04-20T16:49:44.795991Z"},"papermill":{"duration":4.878543,"end_time":"2024-04-18T01:06:01.784082","exception":false,"start_time":"2024-04-18T01:05:56.905539","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# # Load categorical model notebook information\n","# cat_notebook_info = joblib.load('/kaggle/input/homecredit-models-public/other/cat/1/notebook_info.joblib')\n","\n","# # Print notebook information\n","# print(f\"- [cat] notebook_start_time: {cat_notebook_info['notebook_start_time']}\")\n","# print(f\"- [cat] description: {cat_notebook_info['description']}\")\n","\n","# # Load categorical models\n","# cat_models = joblib.load('/kaggle/input/homecredit-models-public/other/cat/1/cat_models.joblib')\n","# cat_models"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:49:49.487608Z","iopub.status.busy":"2024-04-20T16:49:49.487303Z","iopub.status.idle":"2024-04-20T16:49:49.918279Z","shell.execute_reply":"2024-04-20T16:49:49.917458Z","shell.execute_reply.started":"2024-04-20T16:49:49.487583Z"},"papermill":{"duration":0.545928,"end_time":"2024-04-18T01:06:02.349483","exception":false,"start_time":"2024-04-18T01:06:01.803555","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Define the directory path for the train data\n","train_DIR = ROOT / \"parquet_files/train\"\n","\n","# Create a dictionary to store different dataframes generated from reading parquet files\n","data_store = {\n","    # Read the base train data and store it with the key 'df_base'\n","    \"df_base\": read_file(train_DIR / \"train_base.parquet\"),\n","    \n","    # Read depth 0 data, which includes static data and additional files matching a pattern\n","    \"depth_0\": [\n","        read_file(train_DIR / \"train_static_cb_0.parquet\"),\n","        read_files(train_DIR / \"train_static_0_*.parquet\"),\n","    ],\n","    \n","    # Read depth 1 data, including various files related to applicant previous applications, tax registries,\n","    # credit bureau data, and other information\n","    \"depth_1\": [\n","        read_files(train_DIR / \"train_applprev_1_*.parquet\", 1),\n","        read_file(train_DIR / \"train_tax_registry_a_1.parquet\", 1),\n","        read_file(train_DIR / \"train_tax_registry_b_1.parquet\", 1),\n","        read_file(train_DIR / \"train_tax_registry_c_1.parquet\", 1),\n","        read_files(train_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n","        read_file(train_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n","        read_file(train_DIR / \"train_other_1.parquet\", 1),\n","        read_file(train_DIR / \"train_person_1.parquet\", 1),\n","        read_file(train_DIR / \"train_deposit_1.parquet\", 1),\n","        read_file(train_DIR / \"train_debitcard_1.parquet\", 1),\n","    ],\n","    \n","    # Read depth 2 data, which includes additional credit bureau data, applicant previous applications,\n","    # and personal information\n","    \"depth_2\": [\n","        read_file(train_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n","        read_files(train_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n","        read_file(train_DIR / \"train_applprev_2.parquet\", 2),\n","        read_file(train_DIR / \"train_person_2.parquet\", 2)\n","    ]\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:49:49.919808Z","iopub.status.busy":"2024-04-20T16:49:49.919488Z","iopub.status.idle":"2024-04-20T16:49:50.487384Z","shell.execute_reply":"2024-04-20T16:49:50.486432Z","shell.execute_reply.started":"2024-04-20T16:49:49.919782Z"},"papermill":{"duration":0.654075,"end_time":"2024-04-18T01:06:03.010344","exception":false,"start_time":"2024-04-18T01:06:02.356269","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# # Perform feature engineering on the train data using the provided data store\n","df_train = feature_eng(**data_store)\n","\n","# # Print the shape of the train data before further processing\n","print(\"train data shape:\\t\", df_train.shape)\n","\n","# # Clean up memory by deleting the data store and running garbage collection\n","del data_store\n","gc.collect()\n","\n","# # Select columns of interest from the train data\n","# df_train = df_train.select(['case_id'] + cols)\n","\n","# # Convert the train data to a pandas DataFrame and optimize memory usage\n","# df_train, cat_cols = to_pandas(df_train, cat_cols)\n","# df_train = reduce_mem_usage(df_train)\n","\n","# # Set the case_id column as the index of the DataFrame\n","# df_train = df_train.set_index('case_id')\n","\n","# # Print the shape of the train data after processing\n","# print(\"train data shape:\\t\", df_train.shape)\n","\n","# # Run garbage collection to clean up memory\n","# gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:49:50.536945Z","iopub.status.busy":"2024-04-20T16:49:50.536635Z","iopub.status.idle":"2024-04-20T16:49:50.545881Z","shell.execute_reply":"2024-04-20T16:49:50.544997Z","shell.execute_reply.started":"2024-04-20T16:49:50.536919Z"},"papermill":{"duration":0.019084,"end_time":"2024-04-18T01:06:03.118007","exception":false,"start_time":"2024-04-18T01:06:03.098923","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# class VotingModel(BaseEstimator, RegressorMixin):\n","#     def __init__(self, estimators):\n","#         super().__init__()\n","#         self.estimators = estimators\n","        \n","#     def fit(self, X, y=None):\n","#         \"\"\"\n","#         Fit the VotingModel.\n","        \n","#         Parameters:\n","#         - X: array-like or sparse matrix of shape (n_samples, n_features)\n","#             The input samples.\n","#         - y: array-like of shape (n_samples,), default=None\n","#             The target values.\n","            \n","#         Returns:\n","#         - self: object\n","#             Returns self.\n","#         \"\"\"\n","#         return self\n","    \n","#     def predict(self, X):\n","#         \"\"\"\n","#         Predict regression target for X.\n","        \n","#         Parameters:\n","#         - X: array-like or sparse matrix of shape (n_samples, n_features)\n","#             The input samples.\n","            \n","#         Returns:\n","#         - y_preds: array-like of shape (n_samples,)\n","#             The predicted target values.\n","#         \"\"\"\n","#         y_preds = [estimator.predict(X) for estimator in self.estimators]\n","#         return np.mean(y_preds, axis=0)\n","     \n","#     def predict_proba(self, X):      \n","#         \"\"\"\n","#         Predict class probabilities for X.\n","        \n","#         Parameters:\n","#         - X: array-like or sparse matrix of shape (n_samples, n_features)\n","#             The input samples.\n","            \n","#         Returns:\n","#         - proba: array-like of shape (n_samples, n_classes)\n","#             Class probabilities of the input samples.\n","#         \"\"\"\n","#         # lgb\n","#         y_preds = [estimator.predict_proba(X) for estimator in self.estimators[:5]]\n","        \n","#         # cat        \n","#         X[cat_cols] = X[cat_cols].astype(str)\n","#         y_preds += [estimator.predict_proba(X) for estimator in self.estimators[-5:]]\n","        \n","#         return np.mean(y_preds, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:49:50.547322Z","iopub.status.busy":"2024-04-20T16:49:50.547043Z","iopub.status.idle":"2024-04-20T16:49:50.560941Z","shell.execute_reply":"2024-04-20T16:49:50.560135Z","shell.execute_reply.started":"2024-04-20T16:49:50.547297Z"},"papermill":{"duration":0.015923,"end_time":"2024-04-18T01:06:03.142213","exception":false,"start_time":"2024-04-18T01:06:03.12629","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["model = VotingModel(lgb_models + cat_models)\n","len(model.estimators)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:49:50.562667Z","iopub.status.busy":"2024-04-20T16:49:50.562233Z","iopub.status.idle":"2024-04-20T16:49:51.152563Z","shell.execute_reply":"2024-04-20T16:49:51.151631Z","shell.execute_reply.started":"2024-04-20T16:49:50.562636Z"},"papermill":{"duration":0.671929,"end_time":"2024-04-18T01:06:03.821453","exception":false,"start_time":"2024-04-18T01:06:03.149524","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Predict probabilities for the train data\n","y_pred = pd.Series(model.predict_proba(df_train)[:, 1], index=df_train.index)\n","\n","# Read the sample submission file\n","df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n","df_subm = df_subm.set_index(\"case_id\")\n","\n","# Assign predicted probabilities to the submission dataframe\n","df_subm[\"score\"] = y_pred\n","\n","# Save the submission dataframe to a CSV file\n","df_subm.to_csv(\"submission.csv\")\n","\n","# Display the submission dataframe\n","df_subm"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.007392,"end_time":"2024-04-18T01:06:03.836532","exception":false,"start_time":"2024-04-18T01:06:03.82914","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7921029,"sourceId":50160,"sourceType":"competition"},{"modelInstanceId":27710,"sourceId":33095,"sourceType":"modelInstanceVersion"},{"modelInstanceId":27711,"sourceId":33096,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"papermill":{"default_parameters":{},"duration":17.915251,"end_time":"2024-04-18T01:06:04.765569","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-18T01:05:46.850318","version":"2.5.0"}},"nbformat":4,"nbformat_minor":4}
