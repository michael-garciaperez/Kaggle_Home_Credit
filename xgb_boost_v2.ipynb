{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code step by step:\n",
    "\n",
    "1. **Import Libraries**: In the first part, we import all the necessary libraries and modules that we'll be using throughout the code. These include libraries for data manipulation (`numpy`, `pandas`, `polars`), machine learning models (`lightgbm`, `catboost`), file handling (`joblib`, `Path`), and other utilities (`gc`, `glob`).\n",
    "\n",
    "2. **Define Classes for Data Processing**: The code defines two classes: `Pipeline` and `Aggregator`. These classes encapsulate methods for data preprocessing and feature engineering, respectively.\n",
    "\n",
    "3. **Define Functions for Data Reading and Feature Engineering**: Several functions are defined to read data files, perform feature engineering, and convert data to a more memory-efficient format. These functions include `read_file`, `read_files`, `feature_eng`, `to_pandas`, and `reduce_mem_usage`.\n",
    "\n",
    "4. **Load Trained Models and Model Metadata**: The code loads trained models (`lgb_models`, `cat_models`) and their associated metadata (`lgb_notebook_info`, `cat_notebook_info`) from disk. These models are later used for making predictions on the train data.\n",
    "\n",
    "5. **Define train Data Paths and Load train Data**: train data paths are defined, and the train data is loaded using the previously defined functions for reading data files. The loaded train data is stored in a dictionary called `data_store`.\n",
    "\n",
    "6. **Perform Feature Engineering on train Data**: The loaded train data is passed through the feature engineering pipeline (`feature_eng`) to generate features required for making predictions.\n",
    "\n",
    "7. **Generate Predictions**: The `VotingModel` class is used to generate predictions on the train data. This class averages the predictions from multiple individual models to obtain the final prediction probabilities.\n",
    "\n",
    "8. **Save Predictions to Submission File**: The predicted probabilities are saved to a CSV file (`submission.csv`) in the format required for submission. The submission file is based on a sample submission file provided earlier (`sample_submission.csv`).\n",
    "\n",
    "9. **Display Submission DataFrame**: Finally, the submission DataFrame (`df_subm`) is displayed, showing the case IDs and corresponding predicted scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install joblib==1.4.0 numpy==1.26.4 pandas==2.2.2 polars==0.20.21 scikit-learn==1.2.2 lightgbm==4.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm==4.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amahdin/.local/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/amahdin/.local/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib  # Import joblib for saving and loading models\n",
    "from pathlib import Path  # Import Path for working with file paths\n",
    "import gc  # Import gc for garbage collection\n",
    "from glob import glob  # Import glob for file matching\n",
    "import numpy as np  # Import numpy for numerical computing\n",
    "import pandas as pd  # Import pandas for data manipulation\n",
    "import polars as pl  # Import polars for fast data manipulation\n",
    "from sklearn.base import BaseEstimator, RegressorMixin  # Import BaseEstimator and RegressorMixin from sklearn.base\n",
    "from sklearn.metrics import roc_auc_score  # Import roc_auc_score from sklearn.metrics\n",
    "#import lightgbm as lgb  # Import lightgbm for gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 6.044727,
     "end_time": "2024-04-18T01:05:56.081208",
     "exception": false,
     "start_time": "2024-04-18T01:05:50.036481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib  # Import joblib for saving and loading models\n",
    "from pathlib import Path  # Import Path for working with file paths\n",
    "import gc  # Import gc for garbage collection\n",
    "from glob import glob  # Import glob for file matching\n",
    "import numpy as np  # Import numpy for numerical computing\n",
    "import pandas as pd  # Import pandas for data manipulation\n",
    "import polars as pl  # Import polars for fast data manipulation\n",
    "from sklearn.base import BaseEstimator, RegressorMixin  # Import BaseEstimator and RegressorMixin from sklearn.base\n",
    "from sklearn.metrics import roc_auc_score  # Import roc_auc_score from sklearn.metrics\n",
    "#import lightgbm as lgb  # Import lightgbm for gradient boosting\n",
    "\n",
    "import warnings  # Import warnings to ignore warnings\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings\n",
    "\n",
    "USER = 'amahdin'\n",
    "ROOT = Path(f\"/home/{USER}/public/home-credit-credit-risk-model-stability\")  # Define ROOT path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.034893,
     "end_time": "2024-04-18T01:05:56.121806",
     "exception": false,
     "start_time": "2024-04-18T01:05:56.086913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    # Method to set data types for specific columns in a DataFrame\n",
    "    def set_table_dtypes(df):\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "        return df\n",
    "\n",
    "    # Method to handle date columns and calculate time differences\n",
    "    def handle_dates(df):\n",
    "        for col in df.columns:\n",
    "            if col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  # Calculate time differences\n",
    "                df = df.with_columns(pl.col(col).dt.total_days())  # Convert time differences to total days\n",
    "        df = df.drop(\"date_decision\", \"MONTH\")  # Drop unnecessary columns\n",
    "        return df\n",
    "\n",
    "    # Method to filter out columns based on missing values and frequency\n",
    "    def filter_cols(df):\n",
    "        for col in df.columns:\n",
    "            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n",
    "                isnull = df[col].is_null().mean()\n",
    "                if isnull > 0.7:\n",
    "                    df = df.drop(col)  # Drop columns with more than 70% missing values\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n",
    "                freq = df[col].n_unique()\n",
    "                if (freq == 1) | (freq > 200):\n",
    "                    df = df.drop(col)  # Drop columns with only one unique value or more than 200 unique values\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator:\n",
    "    # Method to aggregate numerical features\n",
    "    def num_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]  # Select numerical columns\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Calculate max\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Calculate last\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]  # Calculate mean\n",
    "        expr_median = [pl.median(col).alias(f\"median_{col}\") for col in cols]  # Calculate median\n",
    "        expr_var = [pl.var(col).alias(f\"var_{col}\") for col in cols]  # Calculate variance\n",
    "        return expr_max + expr_last + expr_mean \n",
    "\n",
    "    # Method to aggregate date features\n",
    "    def date_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"D\")]  # Select date columns\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Calculate max\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Calculate last\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]  # Calculate mean\n",
    "        expr_median = [pl.median(col).alias(f\"median_{col}\") for col in cols]  # Calculate median\n",
    "        return expr_max + expr_last + expr_mean \n",
    "\n",
    "    # Method to aggregate string features\n",
    "    def str_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"M\",)]  # Select string columns\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Calculate max\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Calculate last\n",
    "        return expr_max + expr_last\n",
    "\n",
    "    # Method to aggregate other features\n",
    "    def other_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]  # Select other columns\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Calculate max\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Calculate last\n",
    "        return expr_max + expr_last\n",
    "\n",
    "    # Method to aggregate count features\n",
    "    def count_expr(df):\n",
    "        cols = [col for col in df.columns if \"num_group\" in col]  # Select count columns\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Calculate max\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Calculate last\n",
    "        return expr_max + expr_last\n",
    "\n",
    "    # Method to get all aggregation expressions\n",
    "    def get_exprs(df):\n",
    "        exprs = Aggregator.num_expr(df) + \\\n",
    "                Aggregator.date_expr(df) + \\\n",
    "                Aggregator.str_expr(df) + \\\n",
    "                Aggregator.other_expr(df) + \\\n",
    "                Aggregator.count_expr(df)\n",
    "        return exprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 0.032096,
     "end_time": "2024-04-18T01:05:56.159326",
     "exception": false,
     "start_time": "2024-04-18T01:05:56.12723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_file(path, depth=None):\n",
    "    # Read parquet file into a Polars DataFrame\n",
    "    df = pl.read_parquet(path)\n",
    "    # Set table data types using Pipeline method\n",
    "    df = df.pipe(Pipeline.set_table_dtypes)\n",
    "    # Aggregate features if depth is specified\n",
    "    if depth in [1, 2]:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df)) \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(regex_path, depth=None):\n",
    "    chunks = []\n",
    "    # Iterate over files matching the regex pattern\n",
    "    for path in glob(str(regex_path)):\n",
    "        # Read parquet file into a Polars DataFrame\n",
    "        df = pl.read_parquet(path)\n",
    "        # Set table data types using Pipeline method\n",
    "        df = df.pipe(Pipeline.set_table_dtypes)\n",
    "        # Aggregate features if depth is specified\n",
    "        if depth in [1, 2]:\n",
    "            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "        chunks.append(df)\n",
    "    # Concatenate DataFrames and drop duplicate rows based on \"case_id\"\n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\").unique(subset=[\"case_id\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "    # Add month and weekday features based on \"date_decision\"\n",
    "    df_base = df_base.with_columns(\n",
    "        month_decision = pl.col(\"date_decision\").dt.month(),\n",
    "        weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n",
    "    )\n",
    "    # Join additional depth DataFrames\n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "    # Handle dates using Pipeline method\n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)\n",
    "    return df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pandas(df_data, cat_cols=None):\n",
    "    # Convert Polars DataFrame to pandas DataFrame\n",
    "    df_data = df_data.to_pandas()\n",
    "    # Convert categorical columns to category data type\n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "    return df_data, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" \n",
    "    Iterate through all the columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2  # Memory usage before optimization\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if str(col_type)==\"category\":\n",
    "            continue\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            continue\n",
    "    end_mem = df.memory_usage().sum() / 1024**2  # Memory usage after optimization\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "papermill": {
     "duration": 0.722458,
     "end_time": "2024-04-18T01:05:56.898184",
     "exception": false,
     "start_time": "2024-04-18T01:05:56.175726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lgb_notebook_info = joblib.load('/kaggle/input/homecredit-models-public/other/lgb/1/notebook_info.joblib')\n",
    "\n",
    "# # Print notebook information\n",
    "# print(f\"- [lgb] notebook_start_time: {lgb_notebook_info['notebook_start_time']}\")\n",
    "# print(f\"- [lgb] description: {lgb_notebook_info['description']}\")\n",
    "\n",
    "# # Load columns and categorical columns\n",
    "# cols = lgb_notebook_info['cols']\n",
    "# cat_cols = lgb_notebook_info['cat_cols']\n",
    "# print(f\"- [lgb] len(cols): {len(cols)}\")\n",
    "# print(f\"- [lgb] len(cat_cols): {len(cat_cols)}\")\n",
    "\n",
    "# # Load LightGBM models\n",
    "# lgb_models = joblib.load('/kaggle/input/homecredit-models-public/other/lgb/1/lgb_models.joblib')\n",
    "# lgb_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "papermill": {
     "duration": 4.878543,
     "end_time": "2024-04-18T01:06:01.784082",
     "exception": false,
     "start_time": "2024-04-18T01:05:56.905539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load categorical model notebook information\n",
    "# cat_notebook_info = joblib.load('/kaggle/input/homecredit-models-public/other/cat/1/notebook_info.joblib')\n",
    "\n",
    "# # Print notebook information\n",
    "# print(f\"- [cat] notebook_start_time: {cat_notebook_info['notebook_start_time']}\")\n",
    "# print(f\"- [cat] description: {cat_notebook_info['description']}\")\n",
    "\n",
    "# # Load categorical models\n",
    "# cat_models = joblib.load('/kaggle/input/homecredit-models-public/other/cat/1/cat_models.joblib')\n",
    "# cat_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.545928,
     "end_time": "2024-04-18T01:06:02.349483",
     "exception": false,
     "start_time": "2024-04-18T01:06:01.803555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the directory path for the train data\n",
    "train_DIR = ROOT / \"parquet_files/train\"\n",
    "\n",
    "# Create a dictionary to store different dataframes generated from reading parquet files\n",
    "data_store = {\n",
    "    # Read the base train data and store it with the key 'df_base'\n",
    "    \"df_base\": read_file(train_DIR / \"train_base.parquet\"),\n",
    "    \n",
    "    # Read depth 0 data, which includes static data and additional files matching a pattern\n",
    "    \"depth_0\": [\n",
    "        read_file(train_DIR / \"train_static_cb_0.parquet\"),\n",
    "        read_files(train_DIR / \"train_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \n",
    "    # Read depth 1 data, including various files related to applicant previous applications, tax registries,\n",
    "    # credit bureau data, and other information\n",
    "    \"depth_1\": [\n",
    "        read_files(train_DIR / \"train_applprev_1_*.parquet\", 1),\n",
    "        read_file(train_DIR / \"train_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(train_DIR / \"train_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(train_DIR / \"train_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(train_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(train_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(train_DIR / \"train_other_1.parquet\", 1),\n",
    "        read_file(train_DIR / \"train_person_1.parquet\", 1),\n",
    "        read_file(train_DIR / \"train_deposit_1.parquet\", 1),\n",
    "        read_file(train_DIR / \"train_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \n",
    "    # Read depth 2 data, which includes additional credit bureau data, applicant previous applications,\n",
    "    # and personal information\n",
    "    \"depth_2\": [\n",
    "        read_file(train_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(train_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n",
    "        read_file(train_DIR / \"train_applprev_2.parquet\", 2),\n",
    "        read_file(train_DIR / \"train_person_2.parquet\", 2)\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.654075,
     "end_time": "2024-04-18T01:06:03.010344",
     "exception": false,
     "start_time": "2024-04-18T01:06:02.356269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Perform feature engineering on the train data using the provided data store\n",
    "df_train = feature_eng(**data_store)\n",
    "\n",
    "# # Print the shape of the train data before further processing\n",
    "print(\"train data shape:\\t\", df_train.shape)\n",
    "\n",
    "# # Clean up memory by deleting the data store and running garbage collection\n",
    "del data_store\n",
    "gc.collect()\n",
    "\n",
    "# # Select columns of interest from the train data\n",
    "# df_train = df_train.select(['case_id'] + cols)\n",
    "\n",
    "# # Convert the train data to a pandas DataFrame and optimize memory usage\n",
    "# df_train, cat_cols = to_pandas(df_train, cat_cols)\n",
    "# df_train = reduce_mem_usage(df_train)\n",
    "\n",
    "# # Set the case_id column as the index of the DataFrame\n",
    "# df_train = df_train.set_index('case_id')\n",
    "\n",
    "# # Print the shape of the train data after processing\n",
    "# print(\"train data shape:\\t\", df_train.shape)\n",
    "\n",
    "# # Run garbage collection to clean up memory\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.019084,
     "end_time": "2024-04-18T01:06:03.118007",
     "exception": false,
     "start_time": "2024-04-18T01:06:03.098923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class VotingModel(BaseEstimator, RegressorMixin):\n",
    "#     def __init__(self, estimators):\n",
    "#         super().__init__()\n",
    "#         self.estimators = estimators\n",
    "        \n",
    "#     def fit(self, X, y=None):\n",
    "#         \"\"\"\n",
    "#         Fit the VotingModel.\n",
    "        \n",
    "#         Parameters:\n",
    "#         - X: array-like or sparse matrix of shape (n_samples, n_features)\n",
    "#             The input samples.\n",
    "#         - y: array-like of shape (n_samples,), default=None\n",
    "#             The target values.\n",
    "            \n",
    "#         Returns:\n",
    "#         - self: object\n",
    "#             Returns self.\n",
    "#         \"\"\"\n",
    "#         return self\n",
    "    \n",
    "#     def predict(self, X):\n",
    "#         \"\"\"\n",
    "#         Predict regression target for X.\n",
    "        \n",
    "#         Parameters:\n",
    "#         - X: array-like or sparse matrix of shape (n_samples, n_features)\n",
    "#             The input samples.\n",
    "            \n",
    "#         Returns:\n",
    "#         - y_preds: array-like of shape (n_samples,)\n",
    "#             The predicted target values.\n",
    "#         \"\"\"\n",
    "#         y_preds = [estimator.predict(X) for estimator in self.estimators]\n",
    "#         return np.mean(y_preds, axis=0)\n",
    "     \n",
    "#     def predict_proba(self, X):      \n",
    "#         \"\"\"\n",
    "#         Predict class probabilities for X.\n",
    "\n",
    "#         Parameters:\n",
    "#         - X: array-like or sparse matrix of shape (n_samples, n_features)\n",
    "#             The input samples.\n",
    " \n",
    "#         Returns:\n",
    "#         - proba: array-like of shape (n_samples, n_classes)\n",
    "#             Class probabilities of the input samples.\n",
    "#         \"\"\"\n",
    "#         # lgb\n",
    "#         y_preds = [estimator.predict_proba(X) for estimator in self.estimators[:5]]\n",
    "        \n",
    "#         # cat        \n",
    "#         X[cat_cols] = X[cat_cols].astype(str)\n",
    "#         y_preds += [estimator.predict_proba(X) for estimator in self.estimators[-5:]]\n",
    "        \n",
    "#         return np.mean(y_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_columns = [col for col, dtype in df_train.schema.items() if dtype == pl.datatypes.Utf8]\n",
    "print(f\"Columns to drop: {string_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(string_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-logloss:0.17582\ttrain-logloss:0.17602\n",
      "[1]\teval-logloss:0.15838\ttrain-logloss:0.15845\n",
      "[2]\teval-logloss:0.14637\ttrain-logloss:0.14628\n",
      "[3]\teval-logloss:0.13819\ttrain-logloss:0.13793\n",
      "[4]\teval-logloss:0.13258\ttrain-logloss:0.13213\n",
      "[5]\teval-logloss:0.12856\ttrain-logloss:0.12792\n",
      "[6]\teval-logloss:0.12557\ttrain-logloss:0.12471\n",
      "[7]\teval-logloss:0.12342\ttrain-logloss:0.12239\n",
      "[8]\teval-logloss:0.12183\ttrain-logloss:0.12063\n",
      "[9]\teval-logloss:0.12060\ttrain-logloss:0.11924\n",
      "[10]\teval-logloss:0.11966\ttrain-logloss:0.11810\n",
      "[11]\teval-logloss:0.11891\ttrain-logloss:0.11714\n",
      "[12]\teval-logloss:0.11829\ttrain-logloss:0.11632\n",
      "[13]\teval-logloss:0.11777\ttrain-logloss:0.11562\n",
      "[14]\teval-logloss:0.11721\ttrain-logloss:0.11486\n",
      "[15]\teval-logloss:0.11683\ttrain-logloss:0.11428\n",
      "[16]\teval-logloss:0.11647\ttrain-logloss:0.11374\n",
      "[17]\teval-logloss:0.11611\ttrain-logloss:0.11321\n",
      "[18]\teval-logloss:0.11586\ttrain-logloss:0.11281\n",
      "[19]\teval-logloss:0.11567\ttrain-logloss:0.11245\n",
      "[20]\teval-logloss:0.11537\ttrain-logloss:0.11202\n",
      "[21]\teval-logloss:0.11524\ttrain-logloss:0.11168\n",
      "[22]\teval-logloss:0.11505\ttrain-logloss:0.11137\n",
      "[23]\teval-logloss:0.11492\ttrain-logloss:0.11107\n",
      "[24]\teval-logloss:0.11482\ttrain-logloss:0.11075\n",
      "[25]\teval-logloss:0.11464\ttrain-logloss:0.11047\n",
      "[26]\teval-logloss:0.11449\ttrain-logloss:0.11019\n",
      "[27]\teval-logloss:0.11442\ttrain-logloss:0.11001\n",
      "[28]\teval-logloss:0.11436\ttrain-logloss:0.10980\n",
      "[29]\teval-logloss:0.11430\ttrain-logloss:0.10962\n",
      "[30]\teval-logloss:0.11424\ttrain-logloss:0.10940\n",
      "[31]\teval-logloss:0.11419\ttrain-logloss:0.10920\n",
      "[32]\teval-logloss:0.11413\ttrain-logloss:0.10898\n",
      "[33]\teval-logloss:0.11411\ttrain-logloss:0.10879\n",
      "[34]\teval-logloss:0.11406\ttrain-logloss:0.10861\n",
      "[35]\teval-logloss:0.11401\ttrain-logloss:0.10846\n",
      "[36]\teval-logloss:0.11399\ttrain-logloss:0.10824\n",
      "[37]\teval-logloss:0.11394\ttrain-logloss:0.10803\n",
      "[38]\teval-logloss:0.11385\ttrain-logloss:0.10783\n",
      "[39]\teval-logloss:0.11385\ttrain-logloss:0.10766\n",
      "[40]\teval-logloss:0.11382\ttrain-logloss:0.10749\n",
      "[41]\teval-logloss:0.11383\ttrain-logloss:0.10733\n",
      "[42]\teval-logloss:0.11381\ttrain-logloss:0.10717\n",
      "[43]\teval-logloss:0.11378\ttrain-logloss:0.10699\n",
      "[44]\teval-logloss:0.11376\ttrain-logloss:0.10684\n",
      "[45]\teval-logloss:0.11375\ttrain-logloss:0.10668\n",
      "[46]\teval-logloss:0.11375\ttrain-logloss:0.10650\n",
      "[47]\teval-logloss:0.11369\ttrain-logloss:0.10632\n",
      "[48]\teval-logloss:0.11366\ttrain-logloss:0.10622\n",
      "[49]\teval-logloss:0.11364\ttrain-logloss:0.10606\n",
      "[50]\teval-logloss:0.11362\ttrain-logloss:0.10589\n",
      "[51]\teval-logloss:0.11362\ttrain-logloss:0.10573\n",
      "[52]\teval-logloss:0.11361\ttrain-logloss:0.10561\n",
      "[53]\teval-logloss:0.11360\ttrain-logloss:0.10548\n",
      "[54]\teval-logloss:0.11360\ttrain-logloss:0.10536\n",
      "[55]\teval-logloss:0.11359\ttrain-logloss:0.10521\n",
      "[56]\teval-logloss:0.11358\ttrain-logloss:0.10509\n",
      "[57]\teval-logloss:0.11358\ttrain-logloss:0.10500\n",
      "[58]\teval-logloss:0.11356\ttrain-logloss:0.10489\n",
      "[59]\teval-logloss:0.11356\ttrain-logloss:0.10473\n",
      "[60]\teval-logloss:0.11356\ttrain-logloss:0.10456\n",
      "[61]\teval-logloss:0.11353\ttrain-logloss:0.10442\n",
      "[62]\teval-logloss:0.11349\ttrain-logloss:0.10428\n",
      "[63]\teval-logloss:0.11347\ttrain-logloss:0.10420\n",
      "[64]\teval-logloss:0.11346\ttrain-logloss:0.10406\n",
      "[65]\teval-logloss:0.11345\ttrain-logloss:0.10397\n",
      "[66]\teval-logloss:0.11343\ttrain-logloss:0.10385\n",
      "[67]\teval-logloss:0.11343\ttrain-logloss:0.10369\n",
      "[68]\teval-logloss:0.11342\ttrain-logloss:0.10350\n",
      "[69]\teval-logloss:0.11342\ttrain-logloss:0.10339\n",
      "[70]\teval-logloss:0.11340\ttrain-logloss:0.10325\n",
      "[71]\teval-logloss:0.11341\ttrain-logloss:0.10318\n",
      "[72]\teval-logloss:0.11341\ttrain-logloss:0.10305\n",
      "[73]\teval-logloss:0.11336\ttrain-logloss:0.10288\n",
      "[74]\teval-logloss:0.11334\ttrain-logloss:0.10274\n",
      "[75]\teval-logloss:0.11334\ttrain-logloss:0.10262\n",
      "[76]\teval-logloss:0.11332\ttrain-logloss:0.10247\n",
      "[77]\teval-logloss:0.11332\ttrain-logloss:0.10233\n",
      "[78]\teval-logloss:0.11332\ttrain-logloss:0.10222\n",
      "[79]\teval-logloss:0.11329\ttrain-logloss:0.10208\n",
      "[80]\teval-logloss:0.11326\ttrain-logloss:0.10199\n",
      "[81]\teval-logloss:0.11324\ttrain-logloss:0.10191\n",
      "[82]\teval-logloss:0.11324\ttrain-logloss:0.10176\n",
      "[83]\teval-logloss:0.11324\ttrain-logloss:0.10165\n",
      "[84]\teval-logloss:0.11324\ttrain-logloss:0.10152\n",
      "[85]\teval-logloss:0.11326\ttrain-logloss:0.10139\n",
      "[86]\teval-logloss:0.11326\ttrain-logloss:0.10125\n",
      "[87]\teval-logloss:0.11325\ttrain-logloss:0.10114\n",
      "[88]\teval-logloss:0.11323\ttrain-logloss:0.10106\n",
      "[89]\teval-logloss:0.11326\ttrain-logloss:0.10094\n",
      "[90]\teval-logloss:0.11324\ttrain-logloss:0.10085\n",
      "[91]\teval-logloss:0.11325\ttrain-logloss:0.10072\n",
      "[92]\teval-logloss:0.11325\ttrain-logloss:0.10065\n",
      "[93]\teval-logloss:0.11325\ttrain-logloss:0.10055\n",
      "[94]\teval-logloss:0.11328\ttrain-logloss:0.10041\n",
      "[95]\teval-logloss:0.11328\ttrain-logloss:0.10032\n",
      "[96]\teval-logloss:0.11328\ttrain-logloss:0.10022\n",
      "[97]\teval-logloss:0.11327\ttrain-logloss:0.10010\n",
      "[98]\teval-logloss:0.11328\ttrain-logloss:0.10002\n",
      "[99]\teval-logloss:0.11329\ttrain-logloss:0.09989\n",
      "Log Loss: 0.1133\n",
      "AUC: 0.8424\n"
     ]
    }
   ],
   "source": [
    "# Assuming df_train is your DataFrame\n",
    "# and 'target' is the name of your target variable\n",
    "X = df_train.drop(columns='target')\n",
    "y = df_train['target']\n",
    "\n",
    "# Split the data into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use a smaller subset of data if necessary (e.g., if memory is still an issue)\n",
    "# X_train, y_train = X_train.sample(frac=0.5, random_state=42), y_train.sample(frac=0.5, random_state=42)\n",
    "\n",
    "# Create the XGBoost DMatrix for training and testing\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Set parameters for XGBoost\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # Binary classification\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.3,\n",
    "    'eval_metric': 'logloss',\n",
    "    'tree_method': 'hist',  # Use 'hist' or 'gpu_hist' to reduce memory usage\n",
    "    'subsample': 0.8,       # Use a fraction of the data at each step\n",
    "    'colsample_bytree': 0.8 # Use a fraction of features at each step\n",
    "}\n",
    "\n",
    "# Train the model incrementally\n",
    "num_boost_round = 100\n",
    "evals = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "bst = xgb.train(params, dtrain, num_boost_round=num_boost_round, evals=evals)\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "y_pred_proba = bst.predict(dtest)\n",
    "\n",
    "# Evaluate the model using log loss\n",
    "logloss = log_loss(y_test, y_pred_proba)\n",
    "print(f'Log Loss: {logloss:.4f}')\n",
    "\n",
    "# Evaluate the model using AUC\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f'AUC: {auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and info saved to xgboost_model_with_info.joblib\n"
     ]
    }
   ],
   "source": [
    "# Create notebook_info dictionary\n",
    "notebook_info = {\n",
    "    #'notebook_start_time': datetime(2024, 4, 17, 17, 19, 35, 710340),\n",
    "    'description': 'Add notebook info dict to store cols and cat_cols',\n",
    "    'cols': feature_names,  # Use the feature names directly\n",
    "}\n",
    "\n",
    "# Save the model to a separate file\n",
    "bst.save_model(\"xgboost_model.json\")\n",
    "\n",
    "# Add the model file path to the notebook_info dictionary\n",
    "notebook_info['model_file'] = \"xgboost_model.json\"\n",
    "\n",
    "# Save the notebook_info dictionary using joblib\n",
    "joblib_file = \"xgboost_model_with_info.joblib\"\n",
    "joblib.dump(notebook_info, joblib_file)\n",
    "print(f\"Model and info saved to {joblib_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgboost_model_with_info.joblib']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with string data types\n",
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031214144"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7921029,
     "sourceId": 50160,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 27710,
     "sourceId": 33095,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 27711,
     "sourceId": 33096,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17.915251,
   "end_time": "2024-04-18T01:06:04.765569",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-18T01:05:46.850318",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
